---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.3.1
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

```{python}
# %autosave 0

### Package Requirements ###
import pandas as pd
import numpy as np
```

```{python}
### Data Import ###

## Enter file location here ##
infile = 'c:/users/alathan/desktop/StoreData.csv'
infile2 = 'c:/users/alathan/desktop/LocationData.csv'

# Read in the data
store = pd.read_csv(infile, sep = ',')
location = pd.read_csv(infile2, sep = ',')

# make all columns lowercase
store.columns = store.columns.str.lower()
location.columns = location.columns.str.lower()

# Convert datatypes
store['datedimensionid'] = pd.to_datetime(store['datedimensionid'], format='%Y%m')
store['locationdimensionid'] = store['locationdimensionid'].astype(str)
location['locationdimensionid'] = location['locationdimensionid'].astype(str)

# Replace bad characters in column names
location.columns = location.columns.str.replace(' ','')
location.columns = location.columns.str.replace('.','')
location.columns = location.columns.str.replace('-','')
location.columns = location.columns.str.replace('/','')
location.columns = location.columns.str.replace('&','')

store.columns = store.columns.str.replace(' ','')
store.columns = store.columns.str.replace('.','')
store.columns = store.columns.str.replace('-','')
store.columns = store.columns.str.replace('/','')
store.columns = store.columns.str.replace('&','')

store['locationdimensionid'] = store['locationdimensionid'].astype(str)
```

```{python}
### Metadata info ###

# What sort of dates are we looking at?
print(str(len(store['datedimensionid'].unique())) + " months of data")
print(str(len(store['locationdimensionid'].unique())) + " different stores")

# Sort it by date, store, dept
store = store.sort_values(by=['datedimensionid', 'locationdimensionid', 'departmentdimensionid'])#, ascending=1)

# Add open and close date to store info
tmp = store.groupby('locationdimensionid', as_index=0)[['datedimensionid']].min()
location = pd.merge(location, tmp, on=['locationdimensionid','locationdimensionid'])
location = location.rename(columns={'datedimensionid': 'FirstMonth'})

tmp = store.groupby('locationdimensionid', as_index=0)[['datedimensionid']].max()
location = pd.merge(location, tmp, on=['locationdimensionid','locationdimensionid'])
location = location.rename(columns={'datedimensionid': 'LastMonth'})

# Fill in zip and city for locations


# Free the memory; free the people!
tmp = None

```

```{python}
d1 = store[0:int(len(store) / 5) * 1]
d2 = store[int(len(store) / 5) * 1:int(len(store) / 5) * 2]
d3 = store[int(len(store) / 5) * 2:int(len(store) / 5) * 3]
d4 = store[int(len(store) / 5) * 3:int(len(store) / 5) * 4]
d5 = store[int(len(store) / 5) * 4:]

store = None
```

```{python}
exp1 = d1.melt(
    id_vars=['locationdimensionid','departmentdimensionid','datedimensionid']
    , var_name='Category'
    , value_name='Amount'
)
# Remove null values
exp1 = exp1.dropna(subset=['Amount'])
exp1.loc[~(exp1['Category'].isin(['sales', 'acctsreceivable'])),'Amount'] *= -1

exp2 = d2.melt(
    id_vars=['locationdimensionid','departmentdimensionid','datedimensionid']
    , var_name='Category'
    , value_name='Amount'
)
exp2 = exp2.dropna(subset=['Amount'])
exp2.loc[~(exp2['Category'].isin(['sales', 'acctsreceivable'])),'Amount'] *= -1

exp3 = d3.melt(
    id_vars=['locationdimensionid','departmentdimensionid','datedimensionid']
    , var_name='Category'
    , value_name='Amount'
)
exp3 = exp3.dropna(subset=['Amount'])
exp3.loc[~(exp3['Category'].isin(['sales', 'acctsreceivable'])),'Amount'] *= -1

exp4 = d4.melt(
    id_vars=['locationdimensionid','departmentdimensionid','datedimensionid']
    , var_name='Category'
    , value_name='Amount'
)
exp4 = exp4.dropna(subset=['Amount'])
exp4.loc[~(exp4['Category'].isin(['sales', 'acctsreceivable'])),'Amount'] *= -1

exp5 = d5.melt(
    id_vars=['locationdimensionid','departmentdimensionid','datedimensionid']
    , var_name='Category'
    , value_name='Amount'
)
exp5 = exp5.dropna(subset=['Amount'])
exp5.loc[~(exp5['Category'].isin(['sales', 'acctsreceivable'])),'Amount'] *= -1
```

```{python}
exp1 = pd.concat([exp1, exp2,])
exp2 = None
```

```{python}
exp2 = pd.concat([exp3, exp4,])
exp3 = None
exp4 = None
```

```{python}
exp3 = exp5
exp5 = None
```

```{python}
# Improve column Names
exp1 = exp1.rename(columns={
    'datedimensionid': 'ValDate'
    , 'locationdimensionid': 'StoreID'
    , 'departmentdimensionid': 'DeptID'
})
location = location.rename(columns={
    'locationdimensionid': 'storeID'
})

# Output to file. This is a terrible way to do things, but I don't have enough memory so 
exp1.to_csv('c:/users/alathan/desktop/Expenses.txt', index = False, sep = '\t')
exp2.to_csv('c:/users/alathan/desktop/Expenses.txt', index = False, sep = '\t', mode = 'a', header = 0)
exp3.to_csv('c:/users/alathan/desktop/Expenses.txt', index = False, sep = '\t', mode = 'a', header = 0)
```

```{python}
# Fill in NA for cities and states
all_zips = location.groupby(['city_key', 'state_key','zip_key']).size().reset_index().drop(columns=[0])
# Will drop 210 in favor of 56 for zip 53 (53 has more)
location = location.drop(['state_key', 'city_key'], axis=1)
location = location.merge(all_zips.drop(index=210)[['zip_key', 'state_key', 'city_key']])

# Output Store info to file too
location.to_csv('c:/users/alathan/desktop/StoreInfo.txt', header = True, index = False, sep='\t')
```
